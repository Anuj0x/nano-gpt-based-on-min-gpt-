# nano-gpt-based-on-min-gpt-
build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. Created LLM pipeline: they have their own training sets, training algorithms c Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode().
